{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q5_Q6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "npFemIWusYQi"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import xlogy\n",
        "import h5py\n",
        "from autograd import grad\n",
        "from autograd import elementwise_grad as egrad\n",
        "import autograd.numpy as jnp\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTrTMpTwtLXd"
      },
      "source": [
        "class NN_Layer:\n",
        "    def __init__(self, input_size, output_size,activation_fn):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size[1], output_size[1]) / np.sqrt(input_size[1] + output_size[1])\n",
        "        self.bias = np.random.randn(output_size[1]) / np.sqrt(output_size[1])\n",
        "        self.Z = None\n",
        "        self.A = None\n",
        "        self.activation_fn = activation_fn\n",
        "        self.dL_dw = None\n",
        "        self.dL_db = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.Z = jnp.dot(input, self.weights) + jnp.tile(self.bias,(input.shape[0],1))\n",
        "        # print(self.Z.shape)\n",
        "        # print(self.weights.shape)\n",
        "        # print(self.bias.shape)\n",
        "        self.A = self.get_activation_function(self.activation_fn)(self.Z)\n",
        "        # print(self.A)\n",
        "\n",
        "        return self.A\n",
        "\n",
        "    def sigmoid(self,x):  \n",
        "        a = -jnp.array(x,dtype=float)\n",
        "        b = jnp.exp(a)\n",
        "        return 1/(1+b)\n",
        "\n",
        "    def relu(self,x):\n",
        "        return jnp.maximum(x, 0.0)\n",
        "\n",
        "    def softmax(self,x):\n",
        "      return jnp.exp(x)/jnp.sum(jnp.exp(x))\n",
        "\n",
        "    def identity(self,x):\n",
        "      return x\n",
        "\n",
        "    def get_activation_function(self,name):\n",
        "\n",
        "      if name=='relu':\n",
        "        return self.relu\n",
        "      elif name=='sigmoid':\n",
        "        return self.sigmoid\n",
        "      elif name=='softmax':\n",
        "        return self.softmax\n",
        "      elif name=='identity':\n",
        "        return self.identity\n",
        "  \n",
        "    def CrossE_multi(self, A,y):\n",
        "      CrossE = 0\n",
        "      for k in range(n_classes):\n",
        "        CrossE -= jnp.dot((y == k).astype(float),jnp.log(A[:,k]))   \n",
        "      return CrossE\n",
        "    \n",
        "    def rmse(self,A,y):\n",
        "       return jnp.sum((jnp.square(jnp.subtract(A,y.reshape(-1,1)))))/len(A)\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wC0xK7ZJkGy"
      },
      "source": [
        "def load_dig_dataset():\n",
        "    import numpy as np\n",
        "    from sklearn.datasets import load_digits\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import pandas as pd\n",
        "    scaler = MinMaxScaler()\n",
        "    \n",
        "    n_labeled = 5\n",
        "    n_classes = 10\n",
        "    digits = load_digits(n_class=n_classes)  # consider binary case\n",
        "    X = digits.data\n",
        "    X = scaler.fit_transform(X)\n",
        "    y = digits.target\n",
        "\n",
        "    X = pd.DataFrame(X)\n",
        "    y = pd.Series(y)\n",
        "  \n",
        "    return X,y,n_classes\n",
        "X,y,n_classes = load_dig_dataset()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndMei2IDxIfs"
      },
      "source": [
        "def forplusloss(weights, bias, X_train, y_train,network): #for back propagation using autograd\n",
        "  input = X_train\n",
        "  # print(\"HERE\")\n",
        "  for i in range(len(network)):\n",
        "    Z = jnp.dot(input, jnp.array(weights[i])) + jnp.array(jnp.tile(bias[i],(input.shape[0],1)))\n",
        "\n",
        "    A = network[i].get_activation_function(network[i].activation_fn)(Z)\n",
        "    input = A\n",
        "  loss = network[-1].CrossE_multi(A,y_train)\n",
        "  return loss"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf3eEE5LxKJ0"
      },
      "source": [
        "def predict(network, input):\n",
        "  output = input\n",
        "  for layer in network:\n",
        "      output = layer.forward(output)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHQpwN8LpKiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da90f3aa-3dc0-437b-c743-35d7479215bc"
      },
      "source": [
        "\n",
        "accuracies = []\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=3,shuffle=False)\n",
        "kf.split(X)    \n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split train-test\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    X_train, X_test,y_train, y_test = np.array(X_train),np.array(X_test),np.array(y_train),np.array(y_test)\n",
        "    from tqdm import trange\n",
        "\n",
        "\n",
        "    network = [\n",
        "        NN_Layer((X_train.shape[0],X_train.shape[1]), (X_train.shape[0],20),'sigmoid'),\n",
        "        NN_Layer((X_train.shape[0],20),(X_train.shape[0],n_classes),'softmax'),\n",
        "    ]\n",
        "    m =  X_train.shape[0]\n",
        "    epochs =300\n",
        "    learning_rate = 2\n",
        "  \n",
        "    for i in trange(epochs):\n",
        "      j= 0\n",
        "      weights = []\n",
        "      bias=[]\n",
        "      for layer in network:\n",
        "        # print(i,j)\n",
        "        if j==0:\n",
        "          A_value = layer.forward(X_train)\n",
        "          weights.append(layer.weights)\n",
        "          bias.append(layer.bias)\n",
        "        else:\n",
        "          A_value = layer.forward(A_value)\n",
        "          weights.append(layer.weights)\n",
        "          bias.append(layer.bias)\n",
        "        j+=1\n",
        "\n",
        "      dL_dw = egrad(forplusloss,0)(weights,bias,X_train,y_train,network)\n",
        "      dL_db = egrad(forplusloss,1)(weights,bias,X_train,y_train,network)\n",
        "      for i in range(len(network)):\n",
        "        network[i].weights -= learning_rate * dL_dw[i]/len(X_train)\n",
        "        network[i].bias -= learning_rate * dL_db[i]/len(X_train)\n",
        "      \n",
        "    Z = predict(network,X_test)\n",
        "    A = np.argmax(Z,axis=1)\n",
        "\n",
        "    acc = np.mean(A == y_test)\n",
        "    accuracies.append(acc)\n",
        "     \n",
        "print(\"Accuracies for 3 fold model are \",accuracies)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [00:11<00:00, 26.84it/s]\n",
            "100%|██████████| 300/300 [00:11<00:00, 27.02it/s]\n",
            "100%|██████████| 300/300 [00:11<00:00, 26.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracies for 3 fold model are  [0.9515859766277128, 0.9415692821368948, 0.9131886477462438]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCRPL5d5bfPi"
      },
      "source": [
        "import pandas as pd\n",
        "def load_bos_dataset():\n",
        "    import numpy as np\n",
        "    from sklearn.datasets import load_boston\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    boston = load_boston()  # consider binary case\n",
        "    X = boston.data   \n",
        "    X = scaler.fit_transform(X)\n",
        "    y = boston.target\n",
        "    X = pd.DataFrame(X)\n",
        "    y = pd.Series(y)\n",
        "  \n",
        "    return X,y\n",
        "X,y= load_bos_dataset()\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYOibEEpz1ko"
      },
      "source": [
        "def forplusloss(weights, bias, X_train, y_train,network): #for back propagation using autograd\n",
        "  input = X_train\n",
        "  for i in range(len(network)):\n",
        "    Z = jnp.dot(input, jnp.array(weights[i])) + jnp.array(jnp.tile(bias[i],(input.shape[0],1)))\n",
        "\n",
        "    A = network[i].get_activation_function(network[i].activation_fn)(Z)\n",
        "    input = A\n",
        "  loss = network[-1].rmse(A,y_train)\n",
        "  return loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRtaz72uz0wk"
      },
      "source": [
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    return output"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKulthYEdSQ0",
        "outputId": "ad6afe62-39a3-430a-a9a5-d703f90b1d6e"
      },
      "source": [
        "\n",
        "errors = []\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=3,shuffle=False)\n",
        "kf.split(X)    \n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "\n",
        "  from tqdm import trange\n",
        "\n",
        "\n",
        "  network = [\n",
        "      NN_Layer((X_train.shape[0],X_train.shape[1]), (X_train.shape[0],20),'relu'),\n",
        "      NN_Layer((X_train.shape[0],20),(X_train.shape[0],1),'relu'),\n",
        "  ]\n",
        "  m =  X_train.shape[0]\n",
        "  # print(m)\n",
        "  epochs =300\n",
        "  learning_rate = 2\n",
        "\n",
        "  for i in trange(epochs):\n",
        "    j= 0\n",
        "    weights = []\n",
        "    bias=[]\n",
        "    for layer in network:\n",
        "      # print(i,j)\n",
        "      if j==0:\n",
        "        A_value = layer.forward(X_train)\n",
        "        weights.append(layer.weights)\n",
        "        bias.append(layer.bias)\n",
        "\n",
        "      else:\n",
        "        A_value = layer.forward(A_value)\n",
        "        weights.append(layer.weights)\n",
        "        bias.append(layer.bias)\n",
        "      j+=1\n",
        "\n",
        "\n",
        "    dL_dw = egrad(forplusloss,0)(weights,bias,X_train,y_train,network)\n",
        "    dL_db = egrad(forplusloss,1)(weights,bias,X_train,y_train,network)\n",
        "    for i in range(len(network)):\n",
        "      network[i].weights -= learning_rate * dL_dw[i]/len(X_train)\n",
        "      network[i].bias -= learning_rate * dL_db[i]/len(X_train)\n",
        "    \n",
        "  A = predict(network,X_test)\n",
        "  error = (np.square(np.subtract(A,y_test.reshape(-1,1))).mean())**0.5\n",
        "  errors.append(error)\n",
        "\n",
        "print(\"RMSE for 3 fold model are \",errors)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [00:05<00:00, 51.22it/s]\n",
            "100%|██████████| 300/300 [00:05<00:00, 51.19it/s]\n",
            "100%|██████████| 300/300 [00:05<00:00, 51.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE for 3 fold model are  [2.5218012750073453, 2.4725650701693174, 2.4923906103031412]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_qnDxdE9Ex9"
      },
      "source": [
        "Reference: https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65#:~:text=FC%20layers%20are%20the%20most,connected%20to%20every%20output%20neurons"
      ]
    }
  ]
}